{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bob's Discount Furniture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: because some data products are inconsistent (changing JSON formats, etc...), not all code cleanly refactorable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from ExtractedSpectrum import ExtractedSpectrum\n",
    "from nice_tables import LatexTable\n",
    "import xspec_utils as xs\n",
    "\n",
    "%cd results_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_pnerrs(model_dict, comp, par):\n",
    "    \"\"\"Convenience method to extract\n",
    "    value, +ve error, -ve error\n",
    "    from a model dict as returned by xspec_utils.model_dict.\n",
    "    Errors are signed (i.e., -ve error value is < 0)\n",
    "    \n",
    "    Arguments\n",
    "        model_dict: xspec_utils fit dict (i.e., JSON dict)\n",
    "        comp: component name (string)\n",
    "    Returns:\n",
    "        value, +ve error, -ve error three-tuple\n",
    "    \"\"\"\n",
    "    p = model_dict[comp][par]\n",
    "    val = p['value']\n",
    "    pos_err = p['error'][1] - p['value']\n",
    "    neg_err = p['error'][0] - p['value']\n",
    "    return val, pos_err, neg_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate masses from emission measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def volfrac(r1,r2):\n",
    "    \"\"\"Volume fraction of a sphere subtended by an annulus at scaled radii r1, r2.\n",
    "    Requires r1, r2 both in range [0,1] and r1 <= r2.\n",
    "    Will not give sensible answers otherwise.\n",
    "    \n",
    "    This is the result of integral\n",
    "    \\int_{0}^{2\\pi} d\\phi \\int_{r_1}^{r_2} r dr \\int_{-\\sqrt{R^2-r^2}}^{\\sqrt{R^2-r^2}} dz\n",
    "    \"\"\"\n",
    "    return (1 - r1**2)**(3/2) - (1 - r2**2)**(3/2)\n",
    "\n",
    "def pixel2sqarcsec(pixels):\n",
    "    \"\"\"Convert from XMM detector pixels to sq. arcsec\"\"\"\n",
    "    return pixels * 0.05**2\n",
    "\n",
    "def annulus_area(r1,r2):\n",
    "    \"\"\"Yeah\"\"\"\n",
    "    return np.pi * (r2**2 - r1**2)\n",
    "\n",
    "print \"  0-100\\\": {:.3f}\".format(volfrac(0, 0.25))\n",
    "print \"100-200\\\": {:.3f}\".format(volfrac(0.25, 0.50))\n",
    "print \"200-300\\\": {:.3f}\".format(volfrac(0.50, 0.75))\n",
    "print \"300-400\\\": {:.3f}\".format(volfrac(0.75, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THETA_G309 = 6 * (1/60) * np.pi/180  # 6 arcminutes -> radians\n",
    "M_SUN = 1.988e33  # grams (source: wolfram|alpha)\n",
    "M_H = 1.674e-24  # grams (source: wolfram|alpha)\n",
    "D_5KPC = 1.543e22  # 5 kiloparsecs in cm (source: Wolfram|alpha)\n",
    "\n",
    "def density_scale(norm, angular_radius, f_subtend):\n",
    "    \"\"\"Solves for density from XSPEC apec norm (see http://atomdb.org/faq.php)\n",
    "\n",
    "    Assume a spherical, relatively compact source in the sky.\n",
    "    Let $\\eta_0$ be the corrected XSPEC norm (corrected for chip gaps etc)\n",
    "    that describes emission for a fraction f_subtend of this sphere.\n",
    "    Then:\n",
    "\n",
    "        n_h ~ \\sqrt{ \\frac{ 10^{14} \\eta_0 }{ 0.4 \\theta^3 (5 \\unit{kpc}) } } f^{-1/2} d_{5}^{-1/2}\n",
    "\n",
    "    Arguments:\n",
    "        norm (cm^-5): XSPEC emission measure 10^{-14} * \\frac{1}{4\\pi D^2} \\int n_e n_H dV\n",
    "        angular_radius (radians): radius of entire source\n",
    "        f_subtend: fraction of spherical source subtended (range [0,1])\n",
    "    Output:\n",
    "        density estimate, scaled to filling factor 1 and distance 5 kpc\n",
    "        I.e., n_h ~ OUTPUT * f^{-1/2} d_{5}^{-1/2}\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sqrt( 1e14 * norm / (0.4 * angular_radius**3 * D_5KPC) )\n",
    "\n",
    "def nh2mass(n_h_scale, volume):\n",
    "    \"\"\"Solve for emitting mass (again, scaled to 5kpc and f=1)\n",
    "    using expression from Auchettl+ 2015\n",
    "    \n",
    "        M = 1.4 n_H m_H f V\n",
    "    \n",
    "    (not sure where factor 1.4x comes from, guessing metallicity)\n",
    "    Arguments:\n",
    "        n_h_scale (cm^{-3}): density scaled to 5kpc, f=1 from density_scale(...)\n",
    "        volume (cm^3): source volume under consideration _assuming d=5kpc_\n",
    "    Output:\n",
    "        mass estimate, scaled to filling factor 1 and distance 5 kpc\n",
    "        I.e., M ~ OUTPUT * f^{1/2} d_{5}^{+5/2}\n",
    "    \"\"\"\n",
    "    return 1.4 * n_h_scale * M_H * volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Please note: these constant factors are for 0087940201 MOS1 only\n",
    "# But, the process of \"upscaling\" yields norms that are basically\n",
    "# scaled to that expected for the perfect annulus, without pt source removals, chip gaps, etc\n",
    "# and are INDEPENDENT of the exposure. good.\n",
    "for ann in [(0,100), (100,200), (200,300), (300,400)]:\n",
    "    reg = \"ann_{:03d}_{:03d}\".format(*ann)\n",
    "    fd = xs.load_dict('20160708_fourann_center-mg-free_snr_' + reg + '.json')\n",
    "    md = fd['comps']\n",
    "    \n",
    "    extr = ExtractedSpectrum(\"0087940201\", \"mos1S001\", reg)\n",
    "    print reg\n",
    "    print '  backscal / annulus_area = {:.1f} / {:.1f} = {:.5f}'.format(\n",
    "            pixel2sqarcsec(extr.backscal()), annulus_area(*ann),\n",
    "            pixel2sqarcsec(extr.backscal()) / annulus_area(*ann))\n",
    "    \n",
    "    norm = md['vnei']['norm']['value']\n",
    "    norm_corr = (norm * md['constant']['factor']['value']\n",
    "                 * annulus_area(*ann) / pixel2sqarcsec(extr.backscal()))\n",
    "    print '  XSPEC norm:', norm\n",
    "    print '  norm * constant * (annulus_area / backscal):', norm_corr\n",
    "    \n",
    "    nH_scaled = density_scale(norm_corr, THETA_G309, volfrac(ann[0]/400, ann[1]/400))\n",
    "    mass = nh2mass(nH_scaled, volfrac(ann[0]/400, ann[1]/400) * 4/3*np.pi*(THETA_G309 * D_5KPC)**3)\n",
    "    print '  Density scale for f=1, D=5kpc:', density_scale(norm_corr, THETA_G309, volfrac(ann[0]/400, ann[1]/400))\n",
    "    print '  Inferred mass scale (units: Msun):', mass / M_SUN\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = xs.load_dict('20160713_src_bkg_mg.json')\n",
    "extr = ExtractedSpectrum(\"0087940201\", \"mos1S001\", 'src')\n",
    "print 'Integrated source'\n",
    "print '  backscal / annulus_area = {:.1f} / {:.1f} = {:.5f}'.format(\n",
    "        pixel2sqarcsec(extr.backscal()), annulus_area(0, 400),\n",
    "        pixel2sqarcsec(extr.backscal()) / annulus_area(0, 400))\n",
    "\n",
    "norm = fd['1']['snr_src']['vnei']['norm']['value']\n",
    "norm_bounds = fd['1']['snr_src']['vnei']['norm']['error'][0:2]\n",
    "\n",
    "norm_corr = (norm * fd['1']['snr_src']['constant']['factor']['value']\n",
    "             * annulus_area(0, 400) / pixel2sqarcsec(extr.backscal()))\n",
    "\n",
    "nH_scaled = density_scale(norm_corr, THETA_G309, volfrac(ann[0]/400, ann[1]/400))\n",
    "mass = nh2mass(nH_scaled, 4/3*np.pi*(THETA_G309 * D_5KPC)**3)\n",
    "\n",
    "print '  XSPEC norm: {:.5f} (range: [{:5f}, {:5f}])'.format(norm, *norm_bounds)\n",
    "print '  norm * constant * (annulus_area / backscal):', norm_corr\n",
    "print '  Density scale for f=1, D=5kpc:', nH_scaled\n",
    "print '  Inferred mass scale (units: Msun):', mass / M_SUN\n",
    "\n",
    "mg_abund = fd['1']['snr_src']['vnei']['Mg']['value']\n",
    "si_abund = fd['1']['snr_src']['vnei']['Si']['value']\n",
    "s_abund =  fd['1']['snr_src']['vnei']['S']['value']\n",
    "\n",
    "# Abundances from Table 2 of Wilms+ (2000), using estimated ISM abundances\n",
    "print\n",
    "print '  Inferred Mg mass scale, ISM+ejecta (units: Msun): ', (mg_abund / 1.4) * 10**(7.40 - 12) * 24 * mass / M_SUN\n",
    "print '  Inferred Si mass scale, ISM+ejecta (units: Msun): ', (si_abund / 1.4) * 10**(7.27 - 12) * 28 * mass / M_SUN\n",
    "print '  Inferred S  mass scale, ISM+ejecta (units: Msun): ', (s_abund / 1.4) * 10**(7.09 - 12) * 32 * mass / M_SUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft proton parameters as a function of radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = xs.load_dict(\"20160701_fiveann.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latex_hdr = [['Annulus', '2001 MOS', '2001 PN', '2009 MOS']]\n",
    "latex_cols = ['{:s}', 0, 0, 0]\n",
    "ltab = LatexTable(latex_hdr, latex_cols, \"Soft proton power law indices\", prec=2)\n",
    "\n",
    "labels = ['0-100', '100-200', '200-300', '300-400', '400-500']\n",
    "spectra = [np.array([1,3,5]),\n",
    "           np.array([1,3,5]) + 5,\n",
    "           np.array([1,3,5]) + 10,\n",
    "           np.array([1,3,5]) + 15,\n",
    "           np.array([1,3,5]) + 20]\n",
    "spectra = [map(str, x) for x in spectra]\n",
    "\n",
    "for lab, indices in zip(labels, spectra):\n",
    "    ltr = [lab]\n",
    "    for idx in indices:\n",
    "        ltr.append(fd[idx]['sp']['powerlaw']['PhoIndex']['value'])\n",
    "    ltab.add_row(*ltr)\n",
    "\n",
    "print ltab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to see for 0087940201 MOS1 case, but 200-300 powerlaw basically overlaps 400-500.\n",
    "Behavior is broadly consistent with what we expect, actually, which is very reassuring.  Fits also get jumpier going outwards in radius.  But, outer annuli encompass larger annuli -- so the counts should still be pretty comparable.  I would actually expect outer annuli fits to be a bit better constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.logspace(-1, 1)\n",
    "\n",
    "for offset, exposure in enumerate(['0087940201 MOS1', '0087940201 MOS2', '0087940201 PN',\n",
    "                                   '0551000201 MOS1','0551000201 MOS2']):\n",
    "    print exposure\n",
    "    for idx, lab in zip(np.array([1, 6, 11, 16, 21]) + offset,\n",
    "                        ['0-100', '100-200', '200-300', '300-400', '400-500']):\n",
    "        idx = str(idx)\n",
    "        phoindex = fd[idx]['sp']['powerlaw']['PhoIndex']['value']\n",
    "        norm = fd[idx]['sp']['powerlaw']['norm']['value']\n",
    "        print \"{:s}: index {:.2f}, norm {:.2e}\".format(lab, phoindex, norm)\n",
    "        plt.loglog(x, norm * x**(-1 * phoindex), label=lab)\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"Energy (keV)\")\n",
    "    plt.ylabel(r'Effective photons s$^{-1}$ cm$^{-2}$ keV$^{-1}$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vnei fit parameters as a function of radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = xs.load_dict('20160708_fourann_center-mg-free_snr_ann_000_100.json')\n",
    "fd['comps'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fd = xs.load_dict(\"20160701_fiveann.json\")\n",
    "vals = []\n",
    "p_errs = []\n",
    "n_errs = []\n",
    "for idx, model in zip(np.array([1, 6, 11, 16]),\n",
    "                  ['snr_ann_000_100', 'snr_ann_100_200', 'snr_ann_200_300', 'snr_ann_300_400']):\n",
    "    c = fd[str(idx)][model]['vnei']['kT']\n",
    "    vals.append(c['value'])\n",
    "    p_errs.append(c['error'][1] - c['value'])\n",
    "    n_errs.append(abs(c['error'][0] - c['value']))\n",
    "plt.errorbar([50, 150, 250, 350], vals, yerr=[n_errs, p_errs], xerr=50, marker='o', ls='', label='5')\n",
    "\n",
    "vals = []\n",
    "p_errs = []\n",
    "n_errs = []\n",
    "# Old style json dumps\n",
    "for fname in ['20160708_fourann_center-mg-free_snr_ann_000_100.json',\n",
    "              '20160708_fourann_center-mg-free_snr_ann_100_200.json',\n",
    "              '20160708_fourann_center-mg-free_snr_ann_200_300.json',\n",
    "              '20160708_fourann_center-mg-free_snr_ann_300_400.json']:\n",
    "    fd = xs.load_dict(fname)\n",
    "    c = fd['comps']['vnei']['kT']\n",
    "    vals.append(c['value'])\n",
    "    p_errs.append(c['error'][1] - c['value'])\n",
    "    n_errs.append(abs(c['error'][0] - c['value']))\n",
    "print vals, p_errs, n_errs\n",
    "plt.errorbar([48, 148, 248, 348], vals, yerr=[n_errs, p_errs], xerr=50, marker='o', ls='', label='4Mg')\n",
    "\n",
    "fd = xs.load_dict(\"20160708_fourann_center-mg-o-free.json\")\n",
    "vals = []\n",
    "p_errs = []\n",
    "n_errs = []\n",
    "for idx, model in zip(np.array([1, 6, 11, 16]),\n",
    "                  ['snr_ann_000_100', 'snr_ann_100_200', 'snr_ann_200_300', 'snr_ann_300_400']):\n",
    "    c = fd[str(idx)][model]['vnei']['kT']\n",
    "    vals.append(c['value'])\n",
    "    p_errs.append(c['error'][1] - c['value'])\n",
    "    n_errs.append(abs(c['error'][0] - c['value']))\n",
    "plt.errorbar([52, 152, 252, 352], vals, yerr=[n_errs, p_errs], xerr=50, marker='o', ls='', label='4Mg,O')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(-5,405)\n",
    "plt.xlabel(\"Radius (arcseconds)\")\n",
    "plt.ylabel(r'Plasma electron temperature (keV)')\n",
    "plt.savefig('fig_kt_radius.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste code for ionization timescale Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = xs.load_dict(\"20160701_fiveann.json\")\n",
    "vals = []\n",
    "p_errs = []\n",
    "n_errs = []\n",
    "for idx, model in zip(np.array([1, 6, 11, 16]),\n",
    "                  ['snr_ann_000_100', 'snr_ann_100_200', 'snr_ann_200_300', 'snr_ann_300_400']):\n",
    "    c = fd[str(idx)][model]['vnei']['Tau']\n",
    "    vals.append(c['value'])\n",
    "    p_errs.append(c['error'][1] - c['value'])\n",
    "    n_errs.append(abs(c['error'][0] - c['value']))\n",
    "plt.errorbar([50, 150, 250, 350], vals, yerr=[n_errs, p_errs], xerr=50, marker='o', ls='', label='5')\n",
    "\n",
    "vals = []\n",
    "p_errs = []\n",
    "n_errs = []\n",
    "# Old style json dumps\n",
    "for fname in ['20160708_fourann_center-mg-free_snr_ann_000_100.json',\n",
    "              '20160708_fourann_center-mg-free_snr_ann_100_200.json',\n",
    "              '20160708_fourann_center-mg-free_snr_ann_200_300.json',\n",
    "              '20160708_fourann_center-mg-free_snr_ann_300_400.json']:\n",
    "    fd = xs.load_dict(fname)\n",
    "    c = fd['comps']['vnei']['Tau']\n",
    "    vals.append(c['value'])\n",
    "    p_errs.append(c['error'][1] - c['value'])\n",
    "    n_errs.append(abs(c['error'][0] - c['value']))\n",
    "print vals, p_errs, n_errs\n",
    "plt.errorbar([48, 148, 248, 348], vals, yerr=[n_errs, p_errs], xerr=50, marker='o', ls='', label='4Mg')\n",
    "\n",
    "fd = xs.load_dict(\"20160708_fourann_center-mg-o-free.json\")\n",
    "vals = []\n",
    "p_errs = []\n",
    "n_errs = []\n",
    "for idx, model in zip(np.array([1, 6, 11, 16]),\n",
    "                  ['snr_ann_000_100', 'snr_ann_100_200', 'snr_ann_200_300', 'snr_ann_300_400']):\n",
    "    c = fd[str(idx)][model]['vnei']['Tau']\n",
    "    vals.append(c['value'])\n",
    "    p_errs.append(c['error'][1] - c['value'])\n",
    "    n_errs.append(abs(c['error'][0] - c['value']))\n",
    "plt.errorbar([52, 152, 252, 352], vals, yerr=[n_errs, p_errs], xerr=50, marker='o', ls='', label='4Mg,O')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(-5,405)\n",
    "plt.xlabel(\"Radius (arcseconds)\")\n",
    "plt.ylabel(r'Ionization timescale ($\\mathrm{s\\;cm^{-3}}$)')\n",
    "plt.savefig('fig_tau_radius.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source fit parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: just regenerate all the joint source-background fits with new products.\n",
    "Saves fair bit of trouble, should be straightforward.\n",
    "\n",
    "Please note: final table in .tex file has been significantly modified; presentation requires accompanying comment to explain units (had to strip to prevent table from overflowing page width...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = xs.load_dict(\"20160713_src_bkg_mg.json\")\n",
    "md = fd['1']['snr_src']\n",
    "md_xrb = fd['1']['xrb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latex_hdr = [[r'$N_\\mt{H}$', r'($10^{22} \\unit{cm^{-2}}$)'],\n",
    "             [r'$kT$', '(keV)'],\n",
    "             [r'$\\tau$', r'($10^{10} \\unit{s\\;cm^{-3}}$)'],\n",
    "             ['Mg', '(-)'],\n",
    "             ['Si', '(-)'],\n",
    "             ['S', '(-)'],\n",
    "             ['EM (scaled)', r'($10^{-14} \\unit{cm^{-5}}$)'],\n",
    "             [r'$k_B T_{\\mt{local}}$', '(keV)'],  # XRB parameters\n",
    "             [r'$N_\\mt{H}$', r'($10^{22} \\unit{cm^{-2}}$)'],\n",
    "             [r'$k_B T_{\\mt{halo}}$', '(keV)'],\n",
    "             [r'$\\chi^2_{\\mt{red}}$', ''],\n",
    "             [r'$\\chi^2 / (\\mt{dof})$', '']]\n",
    "latex_hdr = np.array(latex_hdr).T\n",
    "\n",
    "latex_cols = [2, 2, 2, 2, 2, 2, 2] + [2, 2, 2] + ['{:.3f}', '{:s}']\n",
    "ltab = LatexTable(latex_hdr, latex_cols, \"Integrated source with Mg free\", prec=2)\n",
    "\n",
    "ltr = []\n",
    "ltr.extend(val_pnerrs(md, 'tbnew_gas', 'nH'))\n",
    "ltr.extend(val_pnerrs(md, 'vnei', 'kT'))\n",
    "ltr.extend(np.array(val_pnerrs(md, 'vnei', 'Tau')) / 1e10)\n",
    "ltr.extend(val_pnerrs(md, 'vnei', 'Mg'))\n",
    "ltr.extend(val_pnerrs(md, 'vnei', 'Si'))\n",
    "ltr.extend(val_pnerrs(md, 'vnei', 'S'))\n",
    "ltr.extend(val_pnerrs(md, 'vnei', 'norm'))\n",
    "\n",
    "ltr.extend(val_pnerrs(md_xrb, 'apec', 'kT'))\n",
    "ltr.extend(val_pnerrs(md_xrb, 'tbnew_gas', 'nH'))\n",
    "ltr.extend(val_pnerrs(md_xrb, 'apec_5', 'kT'))\n",
    "\n",
    "ltr.append(fd['fitStat'] / fd['dof'])\n",
    "ltr.append(\"{:.2f} / {:d}\".format(fd['fitStat'], fd['dof']))\n",
    "\n",
    "ltab.add_row(*ltr)\n",
    "\n",
    "print ltab\n",
    "print val_pnerrs(md, 'vnei', 'norm')  # Didn't format correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% cat \"20160701_src_bkg.tex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four annuli with varied center abundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdictfs = [\"20160708_fourann_center-mg-free_snr_ann_000_100.json\",\n",
    "          \"20160708_fourann_center-mg-o-free_snr_ann_000_100.json\",\n",
    "          \"20160708_fourann_center-mg-ne-free_snr_ann_000_100.json\",\n",
    "          \"20160708_fourann_center-mg-o-ne-free_snr_ann_000_100.json\",\n",
    "          \"20160708_fourann_center-mg-fe-free_snr_ann_000_100.json\",\n",
    "          \"20160712_fourann_center-mg-o-fe-free.json\"\n",
    "          ]\n",
    "# fdictf = fit dict file\n",
    "labels = [\"4Mg\", \"4Mg,O\", \"4Mg,Ne\", \"4Mg,Ne,O\", \"4Mg,Fe\", \"4Mg,O,Fe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val_pnerrs(model_dict, comp, par):\n",
    "    \"\"\"Convenience method to extract\n",
    "    value, +ve error, -ve error\n",
    "    from a model dict as returned by xspec_utils.model_dict.\n",
    "    Errors are signed (i.e., -ve error value is < 0)\n",
    "    \n",
    "    Arguments\n",
    "        model_dict: xspec_utils fit dict (i.e., JSON dict)\n",
    "        comp: component name (string)\n",
    "    Returns:\n",
    "        value, +ve error, -ve error three-tuple\n",
    "    \"\"\"\n",
    "    p = model_dict[comp][par]\n",
    "    val = p['value']\n",
    "    pos_err = p['error'][1] - p['value']\n",
    "    neg_err = p['error'][0] - p['value']\n",
    "    return val, pos_err, neg_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latex_hdr = [['Annulus', ''],\n",
    "             [r'$n_\\mathrm{H}$', r'($10^{22} \\unit{cm^{-2}}$)'],\n",
    "             [r'$kT$', r'(keV)'],\n",
    "             [r'$\\tau$', r'($10^{10} \\unit{s\\;cm^{-3}}$)'],\n",
    "             ['O', '(-)'],\n",
    "             ['Ne', '(-)'],\n",
    "             ['Mg', '(-)'],\n",
    "             ['Si', '(-)'],\n",
    "             ['S', '(-)'],\n",
    "             ['Fe', '(-)'],\n",
    "             [r'$\\chi^2_{\\mt{red}}$', r'$\\chi^2 / (\\mt{dof})$']]\n",
    "latex_hdr = np.array(latex_hdr).T\n",
    "\n",
    "latex_cols = ['{:s}', 2, 2, 2, 2, 2] + 4 * [2] + ['{:s}']  # O, Ne, Mg, Fe; chisqred\n",
    "ltab = LatexTable(latex_hdr, latex_cols, \"G309.2-0.6 annuli fit with errors\", prec=2)\n",
    "\n",
    "for fdictf, lab in zip(fdictfs, labels):\n",
    "    \n",
    "    fd = xs.load_dict(fdictf)\n",
    "    old = \"20160708\" in fdictf\n",
    "    if old:\n",
    "        md = fd['comps']  # Note that old naming convention confused 'comps' with xspec models\n",
    "    else:\n",
    "        md = fd[\"1\"][\"snr_ann_000_100\"]\n",
    "    \n",
    "    ltr = [lab]\n",
    "    ltr.extend(val_pnerrs(md, 'tbnew_gas', 'nH'))\n",
    "    ltr.extend(val_pnerrs(md, 'vnei', 'kT'))\n",
    "    ltr.extend(np.array(val_pnerrs(md, 'vnei', 'Tau')) / 1e10)\n",
    "    ltr.extend(val_pnerrs(md, 'vnei', 'O'))\n",
    "    ltr.extend(val_pnerrs(md, 'vnei', 'Ne'))\n",
    "    ltr.extend(val_pnerrs(md, 'vnei', 'Mg'))\n",
    "    ltr.extend(val_pnerrs(md, 'vnei', 'Si'))\n",
    "    ltr.extend(val_pnerrs(md, 'vnei', 'S'))\n",
    "    ltr.extend(val_pnerrs(md, 'vnei', 'Fe'))\n",
    "    \n",
    "    if old:\n",
    "        ltr.append(\"{:.3f} = {:.2f} / {:d}\".format(fd['fitstat'][1] / fd['dof'], fd['fitstat'][1], fd['dof']))\n",
    "    else:\n",
    "        ltr.append(\"{:.3f} = {:.2f} / {:d}\".format(fd['fitStat'] / fd['dof'], fd['fitStat'], fd['dof']))\n",
    "\n",
    "    ltab.add_row(*ltr)\n",
    "\n",
    "print ltab.__str__().replace('${1.00}^{-1.00}_{-1.00}$', '    ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulate 4 & 5 annuli fit parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, no \"four annulus stock\" fit JSON available; use pre-generated table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nice LaTeX table\n",
    "\n",
    "fd = xs.load_dict(\"20160701_fiveann.json\")\n",
    "rings = []\n",
    "for idx, model in zip([1, 6, 11, 16, 21], ['snr_ann_000_100', 'snr_ann_100_200',\n",
    "                                           'snr_ann_200_300', 'snr_ann_300_400', 'snr_ann_400_500']):\n",
    "    rings.append(fd[str(idx)][model])\n",
    "\n",
    "def ring_val_errs(ring, cname, pname):\n",
    "    p = ring[cname][pname]\n",
    "    val = p['value']\n",
    "    pos_err = p['error'][1] - p['value']\n",
    "    neg_err = p['error'][0] - p['value']\n",
    "    return val, pos_err, neg_err\n",
    "\n",
    "latex_hdr = [['Annulus', ''],\n",
    "             [r'$n_\\mathrm{H}$', r'($10^{22} \\unit{cm^{-2}}$)'],\n",
    "             [r'$kT$', r'(keV)'],\n",
    "             [r'$\\tau$', r'($10^{10} \\unit{s\\;cm^{-3}}$)'],\n",
    "             ['Si', '(-)'],\n",
    "             ['S', '(-)']]\n",
    "latex_hdr = np.array(latex_hdr).T\n",
    "\n",
    "latex_cols = ['{:s}', 2, 2, 2, 2, 2]\n",
    "ltab = LatexTable(latex_hdr, latex_cols, \"Five annulus fit\", prec=2)\n",
    "\n",
    "for ring in rings:\n",
    "    ltr = [ring['name']]\n",
    "    ltr.extend(ring_val_errs(ring, 'tbnew_gas', 'nH'))\n",
    "    ltr.extend(ring_val_errs(ring, 'vnei', 'kT'))\n",
    "    ltr.extend(np.array(ring_val_errs(ring, 'vnei', 'Tau')) / 1e10)\n",
    "    ltr.extend(ring_val_errs(ring, 'vnei', 'Si'))\n",
    "    ltr.extend(ring_val_errs(ring, 'vnei', 'S'))\n",
    "\n",
    "    ltab.add_row(*ltr)\n",
    "\n",
    "print ltab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be regenerated, with correct emission measures shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat \"20160706_fourann_stock.tex\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
